# Source Code Structure

This directory contains the complete ML project source code organized into modular components.

## ğŸ“ Directory Structure

```
src/
â”œâ”€â”€ __init__.py                 # Main package initialization
â”œâ”€â”€ logger.py                   # Logging configuration
â”œâ”€â”€ exception.py                # Custom exception classes
â”œâ”€â”€ utils.py                    # Utility functions
â”‚
â”œâ”€â”€ components/                 # ML Pipeline Components
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_ingestion.py      # Data loading and splitting
â”‚   â”œâ”€â”€ data_transformation.py # Data preprocessing and feature engineering
â”‚   â””â”€â”€ model_trainer.py       # Model training and evaluation
â”‚
â”œâ”€â”€ pipeline/                   # ML Pipelines
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train_pipeline.py      # End-to-end training pipeline
â”‚   â””â”€â”€ predict_pipeline.py    # Prediction pipeline
â”‚
â””â”€â”€ [Legacy modules]           # Original modules (for backward compatibility)
    â”œâ”€â”€ data_preprocessing.py
    â”œâ”€â”€ feature_engineering.py
    â””â”€â”€ model.py
```

## ğŸ”§ Components

### Data Ingestion (`components/data_ingestion.py`)
- Loads raw data from CSV files
- Performs train-test split
- Saves data to artifacts directory
- Validates data quality

### Data Transformation (`components/data_transformation.py`)
- Handles missing values
- Scales numerical features
- Preprocesses categorical features
- Creates preprocessing pipelines
- Saves preprocessor for reuse

### Model Trainer (`components/model_trainer.py`)
- Trains multiple ML models
- Performs hyperparameter tuning using GridSearchCV
- Evaluates model performance
- Selects best model based on accuracy
- Saves trained model

## ğŸš€ Pipelines

### Training Pipeline (`pipeline/train_pipeline.py`)
Orchestrates the complete training workflow:
1. Data Ingestion
2. Data Transformation
3. Model Training

**Usage:**
```python
from src.pipeline import TrainPipeline

pipeline = TrainPipeline()
accuracy = pipeline.run_pipeline(
    data_path="data.csv",
    target_column="target",
    test_size=0.2
)
```

### Prediction Pipeline (`pipeline/predict_pipeline.py`)
Handles predictions on new data:
- Loads trained model and preprocessor
- Transforms input data
- Makes predictions

**Usage:**
```python
from src.pipeline import PredictPipeline, CustomData
import pandas as pd

# Method 1: Using DataFrame
pipeline = PredictPipeline()
predictions = pipeline.predict(df)

# Method 2: Using CustomData
custom_data = CustomData(feature1=1.0, feature2=2.0)
df = custom_data.get_data_as_dataframe()
predictions = pipeline.predict(df)
```

## ğŸ“ Logger

Centralized logging system that:
- Creates timestamped log files in `logs/` directory
- Logs to both file and console
- Captures detailed information (timestamp, line number, level, message)

**Usage:**
```python
from src.logger import logger

logger.info("Information message")
logger.warning("Warning message")
logger.error("Error message")
```

## âš ï¸ Exception Handling

Custom exception classes for better error tracking:
- `CustomException` - Base exception with detailed error info
- `DataValidationError` - Data validation errors
- `ModelTrainingError` - Model training errors
- `DataPreprocessingError` - Preprocessing errors
- `FeatureEngineeringError` - Feature engineering errors
- `ModelLoadingError` - Model loading errors
- `PredictionError` - Prediction errors

**Usage:**
```python
from src.exception import CustomException, DataValidationError
import sys

try:
    # Your code
    pass
except Exception as e:
    raise CustomException(str(e), sys)
```

## ğŸ› ï¸ Utilities

Helper functions in `utils.py`:
- `save_model()` - Save trained models
- `load_model()` - Load trained models
- `load_dataset()` - Load datasets from CSV
- `split_features_target()` - Split features and target
- `get_class_distribution()` - Get class distribution stats
- `check_class_imbalance()` - Check for class imbalance

## ğŸ“¦ Artifacts

All trained models, preprocessors, and data splits are saved to the `artifacts/` directory:
```
artifacts/
â”œâ”€â”€ raw.csv              # Raw data
â”œâ”€â”€ train.csv           # Training data
â”œâ”€â”€ test.csv            # Test data
â”œâ”€â”€ preprocessor.pkl    # Fitted preprocessor
â””â”€â”€ model.pkl           # Trained model
```

## ğŸš« Git Ignore

The following are automatically ignored by git:
- `logs/` directory (created by logger)
- `artifacts/` directory (created during training)
- Python cache files
- Virtual environments

## ğŸ”„ Migration from Legacy Code

The legacy modules (`data_preprocessing.py`, `feature_engineering.py`, `model.py`) are still available for backward compatibility. New development should use the component-based architecture.

